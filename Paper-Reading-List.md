# Paper List

| Category                         | Number of Papers | Number of Codes |
| -------------------------------- | ---------------- | --------------- |
| Video Scene Graph                | 36               | 16             |
| Video Object Grounding           |    54              | 16 |
| Video Temporal Grounding[*TODO*] | -                | -               |



## 1.Video Scene Graph

- 【Survey】TPAMI-22-Scene Graph Generation: A Comprehensive Survey[[paper]](https://arxiv.org/abs/2201.00443)
- **[*]CVPR-22-Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs**[[paper]](https://arxiv.org/abs/2112.04222)[[code]](https://github.com/Dawn-LX/VidSGG-BIG)
- AAAI-22-(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering[[paper]](https://arxiv.org/pdf/2202.09277.pdf)
- AAAI-22-Video as Conditional Graph Hierarchy for Multi-Granular Question Answering[[paper]](https://arxiv.org/abs/2112.06197)[[code]](https://github.com/doc-doc/hqga)
- CVPR-21-Home Action Genome: Cooperative Compositional Action Understanding[[paper]](https://arxiv.org/abs/2105.05226)[[code]](https://github.com/nishantrai18/homage)
- CVPR-21-Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval[[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zeng_Multi-Modal_Relational_Graph_for_Cross-Modal_Video_Moment_Retrieval_CVPR_2021_paper.pdf)
- CVPR-21-Representing Videos as Discriminative Sub-graphs for Action Recognition[[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Representing_Videos_As_Discriminative_Sub-Graphs_for_Action_Recognition_CVPR_2021_paper.pdf)
- CVPR-21-Graph-based High-order Relation Modeling for Long-term Action Recognition[[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf)
- **ICCV-21-Target adaptive context aggregation for video scene graph generation** [[paper]](http://openaccess.thecvf.com/content/ICCV2021/html/Teng_Target_Adaptive_Context_Aggregation_for_Video_Scene_Graph_Generation_ICCV_2021_paper.html)[[code]](https://github.com/MCG-NJU/TRACE)
- **ICCV-21-Spatial-Temporal Transformer for Dynamic Scene Graph Generation**[[paper]](https://arxiv.org/abs/2107.12309)[[ppt]](https://youtu.be/6D3ExjQpbjQ)[[code]](https://github.com/yrcong/STTran)
- **ICCV-21-Detecting Human-Object Relationships in Videos**[[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ji_Detecting_Human-Object_Relationships_in_Videos_ICCV_2021_paper.pdf)
- MM-21-Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition[[paper]](https://arxiv.org/abs/2108.08633)
- MM-21-Video Visual Relation Detection via Iterative Inference[[paper]](https://dl.acm.org/doi/abs/10.1145/3474085.3475263)
- MM-21-Interventional Video Relation Detection[[paper]](https://dl.acm.org/doi/abs/10.1145/3474085.3475540)
- MM-21-Video Relation Detection via Tracklet based Visual Transformer[[paper]](https://arxiv.org/abs/2108.08669)[[code]](https://github.com/dawn-lx/vidvrd-tracklets)
- MM-21-Multi-Perspective Video Captioning
- **[*]CVPR-20-Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences**[[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Where_Does_It_Exist_Spatio-Temporal_Video_Grounding_for_Multi-Form_Sentences_CVPR_2020_paper.pdf)[[ppt]](https://crossminds.ai/video/where-does-it-exist-spatio-temporal-video-grounding-for-multi-form-sentences-5f6e6eb1d81cf36f1a8e3024/)[[dataset]](https://github.com/Guaranteer/VidSTG-Dataset)[No plan to release code]
- CVPR-20-Action Genome: Actions As Compositions of Spatio-Temporal Scene Graphs[[paper]](https://arxiv.org/abs/1912.06992) [[dataset]](https://www.actiongenome.org/#download)
- CVPR-20-Spatio-Temporal Graph for Video Captioning with Knowledge Distillation[[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf)[[Plan to release code]](https://github.com/StanfordVL/STGraph)
- CVPR-20-Beyond Short-Term Snippet: Video Relation Detection with Spatio-Temporal Global Context [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Beyond_Short-Term_Snippet_Video_Relation_Detection_With_Spatio-Temporal_Global_Context_CVPR_2020_paper.pdf) 
- CVPR-20-Ego-Topo: Environment Affordances From Egocentric Video[[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.pdf) [[code]](https://github.com/facebookresearch/ego-topo) [[demo]](https://vision.cs.utexas.edu/projects/ego-topo/demo.html) [[talk]](https://www.youtube.com/watch?v=YTx4co3AIDY)
- ECCV-20-Visual Relation Grounding in Videos [[paper]](https://arxiv.org/abs/2007.08814) [[code]](https://github.com/doc-doc/vRGV)
- IJCAI-20-Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description [[paper]](https://www.ijcai.org/proceedings/2020/0131.pdf)
- MM-20-Relational Graph Learning for Grounded Video Description Generation [[paper]](https://dl.acm.org/doi/10.1145/3394171.3413746) 
- MM-20-Video Relation Detection via Multiple Hypothesis Association [[paper]](https://dl.acm.org/doi/abs/10.1145/3394171.3413764) [[code]](https://github.com/szx503045266/VidVRD-MHA)
- MM-20-LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal Networks for HOI in videos [[paper]](https://arxiv.org/abs/2012.09402) [[code]](https://github.com/praneeth11009/LIGHTEN-Learning-Interactions-with-Graphs-and-Hierarchical-TEmporal-Networks-for-HOI)
- ICML-20-Compositional Video Synthesis with Action Graphs [[paper]](https://arxiv.org/abs/2006.15327)[[code]](https://github.com/roeiherz/AG2Video)
- Arxiv-22-Video Is Graph: Structured Graph Module for Video Action Recognition [[paper]](https://arxiv.org/abs/2110.05904) 
- Arxiv-21-Temporal Contrastive Graph Learning for Video Action Recognition and Retrieval [[paper]](https://arxiv.org/abs/2101.00820) [[code]](https://github.com/YangLiu9208/TCGL)
- WACV-21-We don’t Need Thousand Proposals: Single Shot Actor-Action Detection in Videos [[paper]](https://openaccess.thecvf.com/content/WACV2021/papers/Rana_We_Dont_Need_Thousand_Proposals_Single_Shot_Actor-Action_Detection_in_WACV_2021_paper.pdf) [[code]](https://github.com/aayushjr/ssa2d)
- ICDAR-21-ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos [[paper]](https://arxiv.org/abs/2105.11731) [[code]](https://github.com/coldmanck/VidHOI)
- ICME-21-Spatial-Temporal Human-Object Interaction Detection [[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9428163) 
- TNNLS-21-What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection [[paper]](https://arxiv.org/abs/2107.07154) [[code]](https://github.com/sangminwoo/Temporal-Span-Proposal-Network-VidVRD)
- Neurocomputing-21-3-D Relation Network for visual relation recognition in videos [[paper]](https://www.sciencedirect.com/science/article/pii/S0925231220319299)
- ICMR-20-Human Object Interaction Detection via Multi-level Conditioned Network [[paper]](https://dl.acm.org/doi/abs/10.1145/3372278.3390671)

## 2.Video Object Grounding

- 【Survey】-22-The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions [[paper]](https://arxiv.org/abs/2201.08071)
- 【Survey】-22-A Survey of Temporal Activity Localization via Language in Untrimmed Videos [[paper]](https://ieeexplore.ieee.org/abstract/document/9262795)
- 【Survey】-21-A Survey on Temporal Sentence Grounding in Videos [[paper]](https://arxiv.org/abs/2109.08039)
- 【Survey】-21-A Survey on Natural Language Video Localization [[paper]](https://arxiv.org/abs/2104.00234)
- **CVPR-22-TubeDETR: Spatio-Temporal Video Grounding with Transformers** [[paper]](https://arxiv.org/abs/2203.16434) [[code]](https://github.com/antoyang/TubeDETR)
- CVPR-22-Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning [[paper]](https://arxiv.org/abs/2203.13049) [[code]](https://github.com/yyjmjc/compositional-temporal-grounding)
- CVPR-22-Language as Queries for Referring Video Object Segmentation [[paper]](https://arxiv.org/abs/2201.00487) [[code]](https://github.com/wjn922/referformer)
- **AAAI-22-You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation** [[paper]](https://www.aaai.org/AAAI22Papers/AAAI-1100.LiD.pdf)[[code]](https://github.com/Sparklins/YOFO)
- ICML-21-Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube[[paper]](https://arxiv.org/abs/2004.14338) [[code]](https://github.com/google-research-datasets/i3-video)
- ICLR-21-Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning [[paper]](https://arxiv.org/abs/2103.16564)
- **NIPS-21-Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos** [[paper]](https://arxiv.org/abs/2110.10596)
- **NIPS-21-Grounding Spatio-Temporal Language with Transformers** [[paper]](https://arxiv.org/abs/2106.08858) [[code]](https://github.com/flowersteam/spatio-temporal-language-transformers)
- NIPS-21-MOMA: Multi-Object Multi-Actor Activity Parsing [[paper]](https://proceedings.neurips.cc/paper/2021/file/95688ba636a4720a85b3634acfec8cdd-Paper.pdf) [[project]](https://moma.stanford.edu/#/) [[dataset]](https://moma.stanford.edu/#/download) 
- CVPR-21-Towards More Flexible and Accurate Object Tracking With Natural Language: Algorithms and Benchmark [[paper]](https://arxiv.org/abs/2103.16746) [[code]](https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit)
- CVPR-21-Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos [[paper]](https://arxiv.org/abs/2103.12346) [[code]](https://github.com/SijieSong/CVPR21-Cogrounding_semantic_attention)
- CVPR-21-Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation [[paper]](https://arxiv.org/abs/2007.01951) [[code]](https://github.com/JingHuang81/weak-sup-visual-grounding)
- CVPR-21-Scene-Intuitive Agent for Remote Embodied Visual Grounding [[paper]](https://arxiv.org/abs/2103.12944) 
- CVPR-21-Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning[[paper]](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Iterative_Shrinking_for_Referring_Expression_Grounding_Using_Deep_Reinforcement_Learning_CVPR_2021_paper.html) [[code]](https://github.com/insomnia94/ISREG)
- ICCV-21-STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.pdf)
- **ICCV-21-Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions** [[paper]](https://arxiv.org/abs/2110.03562) [[code]](https://github.com/ShuangLI59/weakly-supervised-human-object-detection-video) [[dataset]](https://shuangli-project.github.io/weakly-supervised-human-object-detection-video/)
- ICCV-21-Explainable Video Entailment with Grounded Visual Evidence [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Explainable_Video_Entailment_With_Grounded_Visual_Evidence_ICCV_2021_paper.pdf) 
- IJCAI-21-Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video [[paper]](https://arxiv.org/abs/2108.03825)
- MM-21-Weakly-Supervised Video Object Grounding via Stable Context Learning [[paper]](https://dl.acm.org/doi/abs/10.1145/3474085.3475245) 
- **[*]NIPS-20-Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding** [[paper]](https://papers.nips.cc/paper/2020/file/d27b95cac4c27feb850aaa4070cc4675-Paper.pdf) 
- CVPR-20-Video Object Grounding Using Semantic Roles in Language Description [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sadhu_Video_Object_Grounding_Using_Semantic_Roles_in_Language_Description_CVPR_2020_paper.pdf) [[code]](https://github.com/TheShadow29/vognet-pytorch)
- CVPR-20-Dense Regression Network for Video Grounding [[paper]](https://arxiv.org/abs/2004.03545)[[code]](https://github.com/Alvin-Zeng/DRN)
- ACL-20-BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues [[paper]](https://arxiv.org/abs/2010.10095) [[code]](https://github.com/salesforce/BiST)
- ECCV-20-Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video [[paper]](https://arxiv.org/abs/1911.10967) [[code]](https://github.com/2020aptx4869lm/Forecasting-Human-Object-Interaction-in-FPV)
- IJCAI-20-Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video Grounding [[paper]](https://arxiv.org/abs/2008.06941)
- IJCAI-20-Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description.[[paper]](https://www.ijcai.org/Proceedings/2020/0131.pdf)
- MM-20-Weakly-Supervised Video Object Grounding by Exploring Spatio-Temporal Contexts [[paper]](https://dl.acm.org/doi/10.1145/3394171.3413610)
- **MM-20-Activity-driven Weakly-Supervised Spatio-Temporal Grounding from Untrimmed Videos** [[paper]](https://dl.acm.org/doi/10.1145/3394171.3413614) 
- MM-20-AsyNCE: Disentangling False-Positives for Weakly-Supervised Video Grounding [[paper]](https://dl.acm.org/doi/abs/10.1145/3474085.3481539) 
- **MM-20-Relational Graph Learning for Grounded Video Description Generation** [[paper]](https://dl.acm.org/doi/10.1145/3394171.3413746) 
- TPAMI-21-Natural Language Video Localization: A Revisit in Span-based Question Answering Framework [[paper]](https://arxiv.org/pdf/2102.13558) 
- TCSVT-21-Human-centric Spatio-Temporal Video Grounding With Visual Transformers [[paper]](https://arxiv.org/abs/2011.05049) [[dataset]](https://github.com/tzhhhh123/HC-STVG)
- TMM-21-Weakly Supervised Temporal Adjacent Network for Language Grounding [[paper]](https://github.com/ycWang9725/WSTAN) [[code]](https://github.com/ycWang9725/WSTAN)
- TCSVT-20-Grounding-Tracking-Integration [[paper]](https://arxiv.org/abs/1912.06316)[[dataset]](https://zyang-ur.github.io/gti/LaSOT_updated.csv) [[demo]](https://www.youtube.com/watch?v=Hex4_UElaS8)
- PR-21-Weakly-supervised video object localization with attentive spatio-temporal correlation[[paper]](https://www.sciencedirect.com/science/article/pii/S0167865521000775)
- Arxiv-22-Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding [[paper]](https://arxiv.org/abs/2203.03838) 
- Arxiv-22-Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding [[paper]](https://arxiv.org/abs/2203.02966)
- Arxiv-22-End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding [[paper]](https://arxiv.org/abs/2203.08013)
- Arxiv-22-Explore and Match: End-to-End Video Grounding with Transformer[[paper]](https://arxiv.org/abs/2201.10168)
- Arxiv-22-Learning Sample Importance for Cross-Scenario Video Temporal Grounding [[paper]](https://arxiv.org/abs/2201.02848) 
- CVPRW-21-Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal Video Grounding [[paper]](https://arxiv.org/abs/2106.10634)
- Arixv-21-Weakly-Supervised Video Object Grounding via Causal Intervention [[paper]](https://arxiv.org/abs/2112.00475)
- Arxiv-21-Learning Visual Affordance Grounding from Demonstration Videos [[paper]](https://arxiv.org/abs/2108.05675)
- Arxiv-21-Decoupled Spatial Temporal Graphs for Generic Visual Grounding [[paper]](https://arxiv.org/abs/2103.10191)
- Arxiv-21-Reconstructing and grounding narrated instructional videos in 3D [[paper]](https://arxiv.org/abs/2109.04409)
- Arxiv-21-C3: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues [[paper]](https://arxiv.org/abs/2106.08914)
- Arxiv-21-A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention [[paper]](https://arxiv.org/abs/2009.11232)
- CVPR-21-Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation [[paper]](https://arxiv.org/abs/2104.02026) [[code]](https://github.com/YapengTian/CCOL-CVPR21)
- Openreview(ICLR Rejected)-21-Action Concept Grounding Network for Semantically-Consistent Video Generation [[paper]](https://openreview.net/forum?id=4_57x7xhymn)
- MM-21-Cross-View Exocentric to Egocentric Video Synthesis [[paer]](https://arxiv.org/abs/2107.03120)
- Arxiv-20-Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos [[paper]](https://arxiv.org/abs/2003.07048)
- KDD-20-Grounding Visual Concepts for Zero-Shot Event Detection and Event Captioning [[paper]](https://dl.acm.org/doi/abs/10.1145/3394486.3403072)



*todo*

## 3.Video Temporal Grounding

- **AAAI-22-Memory-Guided Semantic Learning Network for Temporal Sentence Grounding**

- AAAI-22-Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding

- **AAAI-22-Unsupervised Temporal Video Grounding with Deep Semantic Clustering**

- WACV-22-Learning Temporal Video Procedure Segmentation From an Automatically Collected Large Dataset

- NIPS-21-End-to-end Multi-modal Video Temporal Grounding

- **CVPR-21-Interventional Video Grounding With Dual Contrastive Learning**

- **CVPR-21-Cascaded Prediction Network via Segment Tree for Temporal Video Grounding**

- **CVPR-21-Context-Aware Biaffine Localizing Network for Temporal Sentence Grounding**

- CVPR-21-Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos

- CVPR-21-Visual Grounding in Video for Unsupervised Word Translation

- CVPR-21-[Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.html)

- CVPR-21[Embracing Uncertainty: Decoupling and De-Bias for Robust Temporal Grounding](https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Embracing_Uncertainty_Decoupling_and_De-Bias_for_Robust_Temporal_Grounding_CVPR_2021_paper.html)

- CVPR-21-[Multi-Shot Temporal Event Localization: A Benchmark](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.html)

- CVPR-21-[Action Unit Memory Network for Weakly Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Action_Unit_Memory_Network_for_Weakly_Supervised_Temporal_Action_Localization_CVPR_2021_paper.html)

- CVPR-21-[Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.html)

- **[*]CVPR-21-[Learning Salient Boundary Feature for Anchor-free Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.html)**

- **CVPR-21-Embracing Uncertainty: Decoupling and De-Bias for Robust Temporal Grounding**

- **CVPR-21-[CoLA: Weakly-Supervised Temporal Action Localization With Snippet Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_CoLA_Weakly-Supervised_Temporal_Action_Localization_With_Snippet_Contrastive_Learning_CVPR_2021_paper.html)**

- **CVPR-21-[Structured Multi-Level Interaction Network for Video Moment Localization via Language Query](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Structured_Multi-Level_Interaction_Network_for_Video_Moment_Localization_via_Language_CVPR_2021_paper.html)**

- ICCV-21-VLG-Net: Video-Language Graph Matching Network for Video Grounding

- ICCV-21-Support-Set Based Cross-Supervision for Video Grounding

- ICCV-21- Zero-shot Natural Language Video Localization

- **[*]ICCV-21-Boundary-sensitive Pre-training for Temporal Localization in Videos**

- ICCV-21-Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation

- ACL-21-Parallel Attention Network with Sequence Matching for Video Grounding

- AAAI-21-Proposal-Free Video Grounding with Contextual Pyramid Network

- **[*]AAAI-21-Temporally Grounding Language Queries in Videos by Contextual Boundary-Aware Prediction**

- **[*]AAAI-21-Boundary Proposal Network for Two-Stage Natural Language Video Localization**

- AAAI-21-Dense Events Grounding in Video

- WACV-21-DORi: Discovering Object Relationships for Moment Localization of a Natural Language Query in a Video

- EMNLP-21-Natural Language Video Localization with Learnable Moment Proposals

- EMNLP-21-Relation-aware Video Reading Comprehension for Temporal Language Grounding

- EMNLP-21-On Pursuit of Designing Multi-modal Transformer for Video Grounding

- EMNLP-21-[Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding](https://aclanthology.org/2021.findings-emnlp.9.pdf)

- MM-21-Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval

- CVPR-20-Local-Global Video-Text Interactions for Temporal Grounding

- ECCV-20-Visual Relation Grounding in Videos

- ECCV-20-URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark

- AAAI-20-Tree-Structured Policy Based Progressive Reinforcement Learning for Temporally Language Grounding in Video

- MM-20-Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos

- MM-20-Fine-grained Iterative Attention Network for Temporal Language Localization in Videos

- TIP-21-Local Correspondence Network for Weakly Supervised Temporal Sentence Grounding

- TIP-21-Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos

- TIST-21-Multi-Level Query Interaction for Temporal Language Grounding

- HUMA-21-A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric

- ECCVW-21-End-to-End Dense Video Grounding via Parallel Regression

- CVPRW-20-Co-Learn Sounding Object Visual Grounding and Visually Indicated Sound Separation in A Cycle

- Arxiv-22-A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach

- Arxiv-21-Towards Debiasing Temporal Sentence Grounding in Video

- Arxiv-21-Self-supervised Learning for Semi-supervised Temporal Language Grounding

- Arxiv-20-Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video

- Arxiv-20-Modular Action Concept Grounding in Semantic Video Prediction

- Arxiv-21-MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions

- Arxvi-21-LocFormer: Enabling Transformers to Perform Temporal Moment Localization on Long Untrimmed Videos With a Feature Sampling Approach

- ICML-20-Visual Grounding of Learned Physical Models

- NIPS-21-Low-Fidelity Video Encoder Optimization for Temporal Action Localization

  


