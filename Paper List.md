# Paper List

## 1.Video Scene Graph

- 【Survey】TPAMI-22-Scene Graph Generation: A Comprehensive Survey
- **[*]CVPR-22-Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs**
- AAAI-22-(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering
- AAAI-22-Video as Conditional Graph Hierarchy for Multi-Granular Question Answering
- CVPR-21-Home Action Genome: Cooperative Compositional Action Understanding
- CVPR-21-Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval
- CVPR-21-Representing Videos as Discriminative Sub-graphs for Action Recognition
- CVPR-21-Graph-based High-order Relation Modeling for Long-term Action Recognition
- **ICCV-21-[Target adaptive context aggregation for video scene graph generation](http://openaccess.thecvf.com/content/ICCV2021/html/Teng_Target_Adaptive_Context_Aggregation_for_Video_Scene_Graph_Generation_ICCV_2021_paper.html)**
- **ICCV-21-Spatial-Temporal Transformer for Dynamic Scene Graph Generation**
- **ICCV-21-Detecting Human-Object Relationships in Videos**
- MM-21-Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition
- MM-21-Video Visual Relation Detection via Iterative Inference
- MM-21-Interventional Video Relation Detection
- MM-21-Video Relation Detection via Tracklet based Visual Transformer
- MM-21-Multi-Perspective Video Captioning
- **[*]CVPR-20-Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences**
- CVPR-20-Action Genome: Actions As Compositions of Spatio-Temporal Scene Graphs
- CVPR-20-Spatio-Temporal Graph for Video Captioning with Knowledge Distillation
- CVPR-20-Beyond Short-Term Snippet: Video Relation Detection with Spatio-Temporal Global Context
- CVPR-20-Ego-Topo: Environment Affordances From Egocentric Video[x]
- ECCV-20-Visual Relation Grounding in Videos
- IJCAI-20-Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description
- MM-20-Relational Graph Learning for Grounded Video Description Generation
- MM-20-Video Relation Detection via Multiple Hypothesis Association
- MM-20-LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal Networks for HOI in videos
- ICML-20-Compositional Video Synthesis with Action Graphs
- Arxiv-22-Video Is Graph: Structured Graph Module for Video Action Recognition
- Arxiv-21-Temporal Contrastive Graph Learning for Video Action Recognition and Retrieval
- WACV-21-We don’t Need Thousand Proposals: Single Shot Actor-Action Detection in Videos
- ICDAR-21-ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos
- ICME-21-Spatial-Temporal Human-Object Interaction Detection
- TNNLS-21-What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection
- Neurocomputing-21-3-D Relation Network for visual relation recognition in videos
- ICMR-20-Human Object Interaction Detection via Multi-level Conditioned Network

## 2.Video Object Grounding

- 【Survey】-22-The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions
- 【Survey】-22-A Survey of Temporal Activity Localization via Language in Untrimmed Videos
- 【Survey】-21-A Survey on Temporal Sentence Grounding in Videos
- 【Survey】-21-A Survey on Natural Language Video Localization
- **CVPR-22-[Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs](https://arxiv.org/abs/2112.04222)[重复]**
- **CVPR-22-TubeDETR: Spatio-Temporal Video Grounding with Transformers**
- CVPR-22-Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning
- CVPR-22-Language as Queries for Referring Video Object Segmentation
- **AAAI-22-You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation**
- ICML-21-Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube
- ICLR-21-Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning
- **NIPS-21-Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos**
- **NIPS-21-Grounding Spatio-Temporal Language with Transformers**
- NIPS-21-MOMA: Multi-Object Multi-Actor Activity Parsing
- CVPR-21-Towards More Flexible and Accurate Object Tracking With Natural Language: Algorithms and Benchmark
- CVPR-21-Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos
- CVPR-21-Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation
- CVPR-21-Scene-Intuitive Agent for Remote Embodied Visual Grounding
- CVPR-21-[Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Iterative_Shrinking_for_Referring_Expression_Grounding_Using_Deep_Reinforcement_Learning_CVPR_2021_paper.html)
- ICCV-21-STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding
- **ICCV-21-Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions**
- ICCV-21-Explainable Video Entailment with Grounded Visual Evidence
- IJCAI-21-Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video
- MM-21-Weakly-Supervised Video Object Grounding via Stable Context Learning
- **[*]NIPS-20-Counterfactual Contrastive Learning for Weakly-Supervised Vision-Language Grounding**
- CVPR-20-Video Object Grounding Using Semantic Roles in Language Description
- **CVPR-20-Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences**[重复]
- CVPR-20-Dense Regression Network for Video Grounding
- ECCV-20-Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video
- IJCAI-20-Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video Grounding
- IJCAI-20-[Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description.](https://www.ijcai.org/Proceedings/2020/0131.pdf)
- MM-20-Weakly-Supervised Video Object Grounding by Exploring Spatio-Temporal Contexts
- **MM-20-Activity-driven Weakly-Supervised Spatio-Temporal Grounding from Untrimmed Videos**
- MM-20-AsyNCE: Disentangling False-Positives for Weakly-Supervised Video Grounding
- **MM-20-Relational Graph Learning for Grounded Video Description Generation**
- TPAMI-21-Natural Language Video Localization: A Revisit in Span-based Question Answering Framework
- TCSVT-21-Human-centric Spatio-Temporal Video Grounding With Visual Transformers
- TMM-21-Weakly Supervised Temporal Adjacent Network for Language Grounding
- TCSVT-20-Grounding-Tracking-Integration
- Arxiv-22-Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding
- Arxiv-22-Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding
- Arxiv-22-End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding
- Arxiv-22-Explore and Match: End-to-End Video Grounding with Transformer
- Arxiv-22-Learning Sample Importance for Cross-Scenario Video Temporal Grounding
- CVPRW-21-Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal Video Grounding
- Arixv-21-Weakly-Supervised Video Object Grounding via Causal Intervention
- Arxiv-21-Learning Visual Affordance Grounding from Demonstration Videos
- Arxiv-21-Decoupled Spatial Temporal Graphs for Generic Visual Grounding
- Arxiv-21-Reconstructing and grounding narrated instructional videos in 3D
- Arxiv-21-C3: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues
- Arxiv-21-A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention
- CVPR-21-Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation
- Openreview-21-Action Concept Grounding Network for Semantically-Consistent Video Generation
- MM-21-Cross-View Exocentric to Egocentric Video Synthesis
- Arxiv-20-Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos
- KDD-20-Grounding Visual Concepts for Zero-Shot Event Detection and Event Captioning

## 3.Video Temporal Grounding

- **AAAI-22-Memory-Guided Semantic Learning Network for Temporal Sentence Grounding**

- AAAI-22-Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding

- **AAAI-22-Unsupervised Temporal Video Grounding with Deep Semantic Clustering**

- WACV-22-Learning Temporal Video Procedure Segmentation From an Automatically Collected Large Dataset

- NIPS-21-End-to-end Multi-modal Video Temporal Grounding

- **CVPR-21-Interventional Video Grounding With Dual Contrastive Learning**

- **CVPR-21-Cascaded Prediction Network via Segment Tree for Temporal Video Grounding**

- **CVPR-21-Context-Aware Biaffine Localizing Network for Temporal Sentence Grounding**

- CVPR-21-Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos

- CVPR-21-Visual Grounding in Video for Unsupervised Word Translation

- CVPR-21-[Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.html)

- CVPR-21[Embracing Uncertainty: Decoupling and De-Bias for Robust Temporal Grounding](https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Embracing_Uncertainty_Decoupling_and_De-Bias_for_Robust_Temporal_Grounding_CVPR_2021_paper.html)

- CVPR-21-[Multi-Shot Temporal Event Localization: A Benchmark](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.html)

- CVPR-21-[Action Unit Memory Network for Weakly Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Action_Unit_Memory_Network_for_Weakly_Supervised_Temporal_Action_Localization_CVPR_2021_paper.html)

- CVPR-21-[Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.html)

- **[*]CVPR-21-[Learning Salient Boundary Feature for Anchor-free Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.html)**

- **CVPR-21-Embracing Uncertainty: Decoupling and De-Bias for Robust Temporal Grounding**

- **CVPR-21-[CoLA: Weakly-Supervised Temporal Action Localization With Snippet Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_CoLA_Weakly-Supervised_Temporal_Action_Localization_With_Snippet_Contrastive_Learning_CVPR_2021_paper.html)**

- **CVPR-21-[Structured Multi-Level Interaction Network for Video Moment Localization via Language Query](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Structured_Multi-Level_Interaction_Network_for_Video_Moment_Localization_via_Language_CVPR_2021_paper.html)**

- ICCV-21-VLG-Net: Video-Language Graph Matching Network for Video Grounding

- ICCV-21-Support-Set Based Cross-Supervision for Video Grounding

- ICCV-21- Zero-shot Natural Language Video Localization

- **[*]ICCV-21-Boundary-sensitive Pre-training for Temporal Localization in Videos**

- ICCV-21-Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation

- ACL-21-Parallel Attention Network with Sequence Matching for Video Grounding

- AAAI-21-Proposal-Free Video Grounding with Contextual Pyramid Network

- **[*]AAAI-21-Temporally Grounding Language Queries in Videos by Contextual Boundary-Aware Prediction**

- **[*]AAAI-21-Boundary Proposal Network for Two-Stage Natural Language Video Localization**

- AAAI-21-Dense Events Grounding in Video

- WACV-21-DORi: Discovering Object Relationships for Moment Localization of a Natural Language Query in a Video

- EMNLP-21-Natural Language Video Localization with Learnable Moment Proposals

- EMNLP-21-Relation-aware Video Reading Comprehension for Temporal Language Grounding

- EMNLP-21-On Pursuit of Designing Multi-modal Transformer for Video Grounding

- EMNLP-21-[Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding](https://aclanthology.org/2021.findings-emnlp.9.pdf)

- MM-21-Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval

- CVPR-20-Local-Global Video-Text Interactions for Temporal Grounding

- ECCV-20-Visual Relation Grounding in Videos

- ECCV-20-URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark

- AAAI-20-Tree-Structured Policy Based Progressive Reinforcement Learning for Temporally Language Grounding in Video

- MM-20-Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos

- MM-20-Fine-grained Iterative Attention Network for Temporal Language Localization in Videos

- TIP-21-Local Correspondence Network for Weakly Supervised Temporal Sentence Grounding

- TIP-21-Multi-Modal Interaction Graph Convolutional Network for Temporal Language Localization in Videos

- TIST-21-Multi-Level Query Interaction for Temporal Language Grounding

- HUMA-21-A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric

- ECCVW-21-End-to-End Dense Video Grounding via Parallel Regression

- CVPRW-20-Co-Learn Sounding Object Visual Grounding and Visually Indicated Sound Separation in A Cycle

- Arxiv-22-A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach

- Arxiv-21-Towards Debiasing Temporal Sentence Grounding in Video

- Arxiv-21-Self-supervised Learning for Semi-supervised Temporal Language Grounding

- Arxiv-20-Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video

- Arxiv-20-Modular Action Concept Grounding in Semantic Video Prediction

- Arxiv-21-MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions

- Arxvi-21-LocFormer: Enabling Transformers to Perform Temporal Moment Localization on Long Untrimmed Videos With a Feature Sampling Approach

- ICML-20-Visual Grounding of Learned Physical Models

- NIPS-21-Low-Fidelity Video Encoder Optimization for Temporal Action Localization

  

